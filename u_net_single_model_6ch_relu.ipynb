{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Conv2D,ReLU,UpSampling2D,MaxPool2D,Dropout,BatchNormalization,Layer,Activation\n",
    "from tensorflow.keras import Model,mixed_precision\n",
    "from tensorflow.keras.activations import sigmoid,tanh,leaky_relu\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU,CategoricalCrossentropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"MLIR_CRASH_REPRODUCER_DIRECTORY\"]=\"./crash\"\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory where your images and masks are stored\n",
    "images_dir = \"dataset/images\"\n",
    "mask_directories = [\"BUILDING\", \"ROAD\", \"WATER\", \"NONE\", \"FOREST\", \"LAND\"]\n",
    "modelname=\"unet-128-6ch-L-512-ACT-relu-LOSS-dice+rms-MET-acc+iou\"\n",
    "\n",
    "\n",
    "\n",
    "# List all image filenames\n",
    "image_filenames = os.listdir(images_dir)\n",
    "image_filenames = [i for i in image_filenames if i.endswith(('.png'))]\n",
    "image_filenames.sort()\n",
    "mask_filenames=[i.split(\"_\")[1].split(\".\")[0] for i in image_filenames]\n",
    "print(mask_filenames[:5])\n",
    "image_filenames = [i for i in image_filenames if i.endswith(('.png'))]\n",
    "tar_img=[\n",
    "    '043165',\n",
    "    '007547',\n",
    "    '008390',\n",
    "    '005571',\n",
    "    '060565',\n",
    "    '022764',\n",
    "    '023448',\n",
    "    '001343',\n",
    "    '005403',\n",
    "    ]\n",
    "# tar_mask=[\n",
    "#     '043165',\n",
    "#     '007547',\n",
    "#     '008390',\n",
    "#     '005571',\n",
    "#     '060565',\n",
    "#     '022764',\n",
    "#     '023448',\n",
    "#     '001343',\n",
    "#     '005403',\n",
    "#     ]\n",
    "\n",
    "\n",
    "# Create file paths for images\n",
    "image_paths = [os.path.join(images_dir, filename) for filename in image_filenames]\n",
    "print(image_paths[:3])\n",
    "tar_im_paths = [os.path.join(images_dir, \"im_\"+filename+\".png\") for filename in tar_img]\n",
    "print(tar_im_paths[:3])\n",
    "# Split data into train and test set\n",
    "image_train_dataset, image_test_dataset, mask_train, mask_test= train_test_split(image_paths,mask_filenames, train_size=0.9, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Create TensorFlow dataset for images\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((image_train_dataset,mask_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((image_test_dataset,mask_test))\n",
    "tar_dataset = tf.data.Dataset.from_tensor_slices((tar_im_paths,tar_img))\n",
    "\n",
    "\n",
    "# Function to load image and corresponding masks\n",
    "def load_image_and_masks(image_filename,mask_filename):\n",
    "    # image_path = tf.io.matching_files( image_filename)\n",
    "    # print(image_path)\n",
    "    image = tf.io.read_file(image_filename)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # print(\"Image\",type(image))\n",
    "    \n",
    "    \n",
    "    # mask_filenames = find_mask_filenames(image_filename)\n",
    "    # masks = []\n",
    "    masks=[]\n",
    "    for mask_dir in mask_directories:\n",
    "        r=f\"./dataset/masks/{mask_dir}_BW/msk_\"+mask_filename+\".png\"\n",
    "        mask = tf.io.read_file(r)\n",
    "        mask = tf.image.decode_image(mask, channels=1)  # Read mask as 1 channel\n",
    "        mask = tf.cast(mask, tf.float32) / 255.0\n",
    "        masks.append(mask)\n",
    "    # masks.append(mask)\n",
    "    masks=tf.concat(masks,axis=2)\n",
    "    # Combine masks into a single multi-channel mask\n",
    "    # combined_mask = tf.concat(masks, axis=-1)\n",
    "    \n",
    "    return image, masks, image_filename\n",
    "\n",
    "# Map loading function to image dataset\n",
    "train_dataset = train_dataset.map(load_image_and_masks)\n",
    "test_dataset = test_dataset.map(load_image_and_masks)\n",
    "tar_dataset=tar_dataset.map(load_image_and_masks)\n",
    "\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 1000\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(64)\n",
    "tar_dataset = tar_dataset.batch(1)\n",
    "\n",
    "# Prefetch dataset for better performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "cmaps=[\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (0., 0., 1., 1.)]),\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (0.,1, 1., 1.)]),\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (1, 0., 1., 1.)]),\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (0., 1., 0., 1.)]),\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (1., 1., 0., 1.)]),\n",
    "    colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (1., 0., 0., 1.)]),\n",
    "]\n",
    "\n",
    "for images,masks, name in train_dataset.take(1):\n",
    "    # Your training code here\n",
    "    # for n-th whole masks\n",
    "    print(masks[0].shape)\n",
    "    # for n-th 1 layer mask\n",
    "    print(masks[0][:,:,0].shape)\n",
    "    for i in range(5):\n",
    "        fig, (ax1,*am) = plt.subplots(1,7)\n",
    "        fig.set_size_inches(16,8)\n",
    "        ax1.imshow(images[i])\n",
    "        ax1.axis(\"off\")\n",
    "        # ax1.set_title(f\"Image {name[i]}\")\n",
    "        for k in range(0,6):\n",
    "            am[k].imshow(masks[i,:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "            am[k].set_title(f\"[{mask_directories[k]}]\")\n",
    "            am[k].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "for images,masks,name in tar_dataset.take(len(tar_img)):\n",
    "    # Your training code here\n",
    "    # for n-th whole masks\n",
    "    print(masks[0].shape)\n",
    "    # for n-th 1 layer mask\n",
    "    print(masks[0][:,:,0].shape)\n",
    "    \n",
    "    for i in range(1):\n",
    "        fig, (ax1,*am) = plt.subplots(1,7)\n",
    "        fig.set_size_inches(16,8)\n",
    "        ax1.imshow(images[i])\n",
    "        ax1.axis(\"off\")\n",
    "        # ax1.set_title(f\"Image {name[i]}\")\n",
    "        for k in range(0,6):\n",
    "            # am[k].imshow(masks[i,:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "            am[k].imshow(masks[i,:,:,k],cmap=colors.LinearSegmentedColormap.from_list(name='Trans2Blue', colors=[(0., 0., 0., 1.), (1., 1., 1., 1.)]),vmin=0.,vmax=1.)\n",
    "            am[k].set_title(f\"[{mask_directories[k]}]\")\n",
    "            am[k].axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_upsample_layer(filters, kernel_size, padding='same', kernel_initializer='he_normal'):\n",
    "    \"\"\" This function creates a layer that upsamples an input tensor using a convolutional, batch\n",
    "    normalization, and ReLU activation, followed by an upsampling operation. \"\"\"\n",
    "    def layer(x):\n",
    "        x = Conv2D(filters, kernel_size, padding=padding, kernel_initializer=kernel_initializer)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = ReLU()(x)\n",
    "        # somehow we try tanh\n",
    "        # x = Activation(tanh)(x)\n",
    "        # x = Activation(leaky_relu)(x)\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "def create_downsample_layer(filters, kernel_size, padding='same', kernel_initializer='he_normal'):\n",
    "    \"\"\" This function creates a layer that downsamples an input tensor using a convolutional, batch\n",
    "    normalization, and ReLU activation, followed by a max pooling operation. \"\"\"\n",
    "    def layer(x):\n",
    "        x = Conv2D(filters, kernel_size, padding=padding, kernel_initializer=kernel_initializer)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        # somehow we try tanh\n",
    "        # x = Activation(tanh)(x)\n",
    "        # x = Activation(leaky_relu)(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "dropout_rate = 0.15\n",
    "\n",
    "inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "# # Encoder part of the DeepUNet\n",
    "x = create_downsample_layer(64, 6, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "x = create_downsample_layer(128, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(256, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(512, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "# x = create_downsample_layer(1024, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# x = create_downsample_layer(2048, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "\n",
    "# # Decoder part of the DeepUNet\n",
    "# x = create_upsample_layer(1024, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# x = create_upsample_layer(512, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(256, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(128, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(64, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(32, 6, padding='same', kernel_initializer='he_normal')(x)\n",
    "outputs = Conv2D(6, 1, padding = 'same', activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.load_weights(f\"models/{modelname}.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_mean_io_u', patience=90, verbose=1,\n",
    "                           mode='max', restore_best_weights=True)\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50,min_lr=1e-6,\n",
    "#                               verbose=1, mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=90,min_lr=1e-6,\n",
    "                              verbose=1, mode='min')\n",
    "\n",
    "adam=Adam()\n",
    "\n",
    "# filepath = \"checkpoints/unet-128-6ch-{epoch:03d}-{loss:.4f}.keras\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=adam, model=model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=f\"checkpoints/{modelname}/\", max_to_keep=10)\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "\n",
    "training_cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "metrics_file_name=f'metrics/{modelname}.csv'\n",
    "\n",
    "\n",
    "try:\n",
    "    metrics_df = pd.read_csv(metrics_file_name)\n",
    "    print(metrics_df)\n",
    "    training_cnt=metrics_df['epoch'].max() if not np.isnan(metrics_df['epoch'].max()) else 0\n",
    "    max_iou=metrics_df['val_iou'].max() if not np.isnan(metrics_df['val_iou'].max()) else 0.0\n",
    "    max_acc=metrics_df['val_acc'].max() if not np.isnan(metrics_df['val_acc'].max()) else 0.0\n",
    "    min_loss=metrics_df['val_loss'].min() if not np.isnan(metrics_df['val_loss'].max()) else float('inf')\n",
    "except:\n",
    "    data = {'epoch': [],\n",
    "    'val_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_iou': []}\n",
    "    metrics_df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    metrics_df.to_csv(metrics_file_name, index=False)\n",
    "    training_cnt=0\n",
    "    max_iou=0.0\n",
    "    max_acc=0.0\n",
    "    min_loss=float('inf')\n",
    "metrics_df = metrics_df.astype({\"epoch\": int})\n",
    "data_template={'epoch': 0,\n",
    "    'val_acc': 0.0,\n",
    "    'val_loss': 0.0,\n",
    "    'val_iou': 0.0}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(status,manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    # Flatten the predictions and ground truth\n",
    "\n",
    "    y_true_flat = tf.reshape(y_true,  [-1, tf.reduce_prod(tf.shape(y_true)[2:])])\n",
    "\n",
    "    # y_true_flat = tf.reshape(y_true,  [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred,  [-1, tf.reduce_prod(tf.shape(y_pred)[2:])])\n",
    "    # y_pred_flat = tf.reshape(y_pred,  [-1])\n",
    "    # print(y_true.shape)\n",
    "    # print(y_true_flat.shape)\n",
    "\n",
    "    # Compute the intersection and union\n",
    "    \n",
    "    # intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat,axis=[0])+1e-6\n",
    "    # union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat)\n",
    "    union = tf.reduce_sum(y_true_flat,axis=[0]) + tf.reduce_sum(y_pred_flat,axis=[0])+1e-6\n",
    "    \n",
    "\n",
    "    # Compute the Dice loss\n",
    "    dice_loss = 1 - 2 * intersection / union\n",
    "    dice_loss=tf.reduce_sum(tf.math.abs(dice_loss))\n",
    "    print(intersection,dice_loss)\n",
    "\n",
    "    return dice_loss\n",
    "\n",
    "def dice_loss_w_rms(y_true, y_pred):\n",
    "    # Flatten the predictions and ground truth\n",
    "\n",
    "    y_true_flat = tf.reshape(y_true,  [-1, tf.reduce_prod(tf.shape(y_true)[2:])])\n",
    "    # y_true_flat = tf.reshape(y_true,  [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred,  [-1, tf.reduce_prod(tf.shape(y_pred)[2:])])\n",
    "    # y_pred_flat = tf.reshape(y_pred,  [-1])\n",
    "    # print(y_true.shape)\n",
    "    # print(y_true_flat.shape)\n",
    "    sq_dif=tf.square(y_true_flat-y_pred_flat)\n",
    "    reduce_sq_dif=tf.reduce_mean(sq_dif)\n",
    "    sqrt=tf.sqrt(reduce_sq_dif)\n",
    "\n",
    "    # Compute the intersection and union\n",
    "    \n",
    "    # intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat,axis=[0])+1e-6\n",
    "    # union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat)\n",
    "    union = tf.reduce_sum(y_true_flat,axis=[0]) + tf.reduce_sum(y_pred_flat,axis=[0])+1e-6\n",
    "    \n",
    "\n",
    "    # Compute the Dice loss\n",
    "    dice_loss = 1 - 2 * intersection / union\n",
    "    dice_loss=tf.reduce_sum(tf.math.abs(dice_loss))\n",
    "    print(intersection,dice_loss)\n",
    "\n",
    "    return dice_loss+sqrt\n",
    "    # return sqrt\n",
    "\n",
    "\n",
    "iou = MeanIoU(num_classes=6)  # One for all classes\n",
    "# categorical_accuracy = CategoricalCrossentropy(from_logits=True)\n",
    "# alpha = 0.7  # Weight for cross-entropy loss\n",
    "# beta = 1 - alpha  # Weight for Dice loss\n",
    "# dice_loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "# total_loss = alpha * losses.categorical_crossentropy(from_logits=True) + beta * dice_loss\n",
    "\n",
    "\n",
    "# Compile the model with the Dice loss\n",
    "# model.compile(loss=[dice_loss,dice_loss,dice_loss,dice_loss,dice_loss,dice_loss], optimizer='adam', metrics=['accuracy','accuracy','accuracy','accuracy','accuracy','accuracy'])\n",
    "model.compile(loss=[dice_loss_w_rms], optimizer=adam, metrics=[\"accuracy\",iou])\n",
    "\n",
    "for a, b, name in test_dataset.take(1):\n",
    "# test,t=test_dataset.take(1)\n",
    "    print(type(a),len(b))\n",
    "    im_test=a\n",
    "    msk_test=b\n",
    "    im_name=name\n",
    "    print(msk_test[0].shape)\n",
    "print(dice_loss(msk_test[2],msk_test[1]))\n",
    "print(dice_loss_w_rms(msk_test[1],msk_test[2]))\n",
    "# dice_loss(msk_test[0,:,:,0],msk_test[0,:,:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for a, b, name in test_dataset.take(1):\n",
    "# test,t=test_dataset.take(1)\n",
    "    print(type(a),len(b))\n",
    "    im_test=a\n",
    "    msk_test=b\n",
    "    im_name=name\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint,sample\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 8\n",
    "# Set seed\n",
    "# tf.random.set_seed(42)\n",
    "# tf.random.set_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "def process_history(history):\n",
    "    global training_cnt\n",
    "    global metrics_df\n",
    "    for i in history.epoch:\n",
    "        training_cnt+=1\n",
    "        data_template['epoch']=training_cnt\n",
    "        data_template['val_acc']=history.history['val_accuracy'][i]\n",
    "        data_template['val_loss']=history.history['val_loss'][i]\n",
    "        data_template['val_iou']=history.history['val_mean_io_u'][i]\n",
    "        \n",
    "\n",
    "        metrics_df=pd.concat([metrics_df, pd.DataFrame([data_template])], ignore_index=True)\n",
    "    metrics_df.to_csv(metrics_file_name, index=False)\n",
    "\n",
    "class LogWriter:\n",
    "    def __init__(self):\n",
    "        # self.log_file = log_file\n",
    "        self.log_queue = Queue()\n",
    "        self.thread = threading.Thread(target=self._write_logs)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "\n",
    "    def _write_logs(self):\n",
    "        global metrics_df\n",
    "        while True:\n",
    "            hist = self.log_queue.get()\n",
    "            if hist is None:  # Sentinel to stop the thread\n",
    "                break\n",
    "            # with open(self.log_file, 'a') as f:\n",
    "            #     f.write(message + \"\\n\")\n",
    "            metrics_df=pd.concat([metrics_df, pd.DataFrame([hist])], ignore_index=True)\n",
    "            metrics_df.to_csv(metrics_file_name, index=False)\n",
    "            self.log_queue.task_done()\n",
    "\n",
    "    def write_log(self, message):\n",
    "        self.log_queue.put(message)\n",
    "\n",
    "    def stop(self):\n",
    "        self.log_queue.put(None)\n",
    "        self.thread.join()\n",
    "\n",
    "log_writer=LogWriter()\n",
    "\n",
    "\n",
    "class save_if_better_cb(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global max_iou,min_loss\n",
    "        global training_cnt\n",
    "        global metrics_df\n",
    "        global manager\n",
    "        global data_template\n",
    "        global log_writer\n",
    "        training_cnt+=1\n",
    "        data_template['epoch']=training_cnt\n",
    "        data_template['val_acc']=logs.get(\"val_accuracy\")\n",
    "        data_template['val_loss']=logs.get('val_loss')\n",
    "        data_template['val_iou']=logs.get('val_mean_io_u')\n",
    "        log_writer.write_log(data_template)\n",
    "        # if logs.get('val_mean_io_u') > max_iou or logs.get('val_loss') < min_loss:\n",
    "        #     max_iou=logs.get('val_mean_io_u')\n",
    "        #     manager.save()\n",
    "        #     print(f\"\\n\\nEpoch {training_cnt} is better by {max_iou}, Saving the model...\\n\\n\")\n",
    "        #     model_path=f\"models/{modelname}.keras\"\n",
    "        #     model.save(model_path)\n",
    "            \n",
    "\n",
    "        # metrics_df=pd.concat([metrics_df, pd.DataFrame([data_template])], ignore_index=True)\n",
    "        # metrics_df.to_csv(metrics_file_name, index=False)\n",
    "\n",
    "        # print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "\n",
    "\n",
    "def show_picture_compare(original, res, msk, name):\n",
    "    global training_cnt\n",
    "    main=\"\"\"\n",
    "        ........\n",
    "        XXABCDEF\n",
    "        XXGHIJKL\n",
    "        ........\n",
    "        \"\"\"\n",
    "    fig, am = plt.subplot_mosaic(main)\n",
    "    fig.set_size_inches(16,8)\n",
    "    plt.subplots_adjust(hspace=0.25)\n",
    "    am[\"X\"].set_title(f\"Image {name.split('/')[-1]}\")\n",
    "\n",
    "    am[\"X\"].imshow(original)    \n",
    "\n",
    "    for k in range(0,6):\n",
    "        am[chr(ord(\"A\")+k)].imshow(msk[:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "        am[chr(ord(\"A\")+k)].set_title(f\"[{mask_directories[k]}]\")\n",
    "        am[chr(ord(\"G\")+k)].imshow(res[:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "        # am[chr(ord(\"G\")+k)].set_title(f\"[Pred]\")\n",
    "        # plt.tight_layout()\n",
    "    \n",
    "    cmp_path=f'compare_img/{modelname}/{name.split(\"/\")[-1].split(\".\")[0]}/'\n",
    "    if not os.path.exists(cmp_path):\n",
    "        os.makedirs(cmp_path)\n",
    "    plt.savefig(f'{cmp_path}{training_cnt}.png',bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "class write_im_perf(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global max_iou,min_loss\n",
    "        global training_cnt\n",
    "        global metrics_df\n",
    "        global manager\n",
    "        global model\n",
    "        global tar_dataset\n",
    "        if training_cnt%5==0:\n",
    "            for im,msk,im_name in tar_dataset.take(len(tar_img)):\n",
    "                # print(im_test[idx].shape)\n",
    "                \n",
    "                ori=im[0]\n",
    "                res=model.predict(ori[None,...])[0]\n",
    "                msk=msk[0]\n",
    "                name=im_name[0]\n",
    "                name=str(name.numpy()).replace(\"'\",\"\")\n",
    "                show_picture_compare(ori, res, msk, name)\n",
    "\n",
    "        # print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "for images, masks, _ in train_dataset.take(70):\n",
    "\n",
    "    # x,y=preprocess_images_and_masks(dataset_path,i*1000,1000)\n",
    "    # train_images,train_masks,val_images,val_masks,test_images,test_masks=split(x,y)\n",
    "    print(f\"Take {training_cnt}\")\n",
    "    # training_cnt+=1\n",
    "    # print(masks[0].shape)\n",
    "\n",
    "\n",
    "    # Train the UNet model on the training data\n",
    "    history = model.fit(#dataset,\n",
    "                        images,\n",
    "                        masks,\n",
    "                        batch_size=batch_size, epochs=num_epochs,\n",
    "                        # callbacks=[early_stop, reduce_lr],\n",
    "                        callbacks=[early_stop,save_if_better_cb(),write_im_perf()],\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1)\n",
    "    manager.save()\n",
    "    model_path=f\"models/{modelname}.keras\"\n",
    "    model.save(model_path)\n",
    "    epochs_list = list(range(1, len(history.history['val_loss']) + 1))\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_accuracy_k = history.history.keys()\n",
    "\n",
    "    # process_history(history)\n",
    "\n",
    "    # Plot the epoch vs val_loss\n",
    "\n",
    "    # Create a second y-axis for the val_accuracy\n",
    "    # ax2 = ax1.twinx()\n",
    "    # ax2.plot(epochs_list, val_loss, 'k-')\n",
    "    # ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "    # Set the same scaling on the y-axes\n",
    "    # ax1.set_ylim([0.0, 1])\n",
    "    # ax2.set_ylim([0.0, 1])\n",
    "    # idx_list=[randint(0,63) for _ in range(5)]\n",
    "    \n",
    "    # show im\n",
    "\n",
    "    # idx_list=sample(range(0,63),5)\n",
    "    # for idx in idx_list:\n",
    "    #     # print(im_test[idx].shape)\n",
    "        \n",
    "    #     ori=im_test[idx]\n",
    "    #     res=model.predict(ori[None,...])[0]\n",
    "    #     msk=msk_test[idx]\n",
    "    #     name=im_name[idx]\n",
    "    #     name=str(name.numpy()).replace(\"'\",\"\")\n",
    "    #     show_picture_compare(ori, res, msk, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.epoch\n",
    "name.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list=[randint(0,63) for _ in range(10)]\n",
    "for idx in idx_list:\n",
    "    print(im_test[idx].shape)\n",
    "    ori=im_test[idx]\n",
    "    msk=msk_test[idx]\n",
    "    main=\"\"\"\n",
    "    ........\n",
    "    XXABCDEF\n",
    "    XXGHIJKL\n",
    "    ........\n",
    "    \"\"\"\n",
    "    fig, am = plt.subplot_mosaic(main)\n",
    "    fig.set_size_inches(16,8)\n",
    "    # fig.set_constrained_layout(True)\n",
    "    \n",
    "    # print(msk.shape)\n",
    "    # for n-th 1 layer mask\n",
    "    # print(msk)\n",
    "    res=r=model.predict(ori[None,...])[0]\n",
    "    am[\"X\"].imshow(im_test[idx])    \n",
    "\n",
    "    for k in range(0,6):\n",
    "        am[chr(ord(\"A\")+k)].imshow(msk[:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "        am[chr(ord(\"A\")+k)].set_title(f\"True Mask[{k}]\")\n",
    "        am[chr(ord(\"G\")+k)].imshow(res[:,:,k],cmap=cmaps[k],vmin=0.,vmax=1.)\n",
    "        am[chr(ord(\"G\")+k)].set_title(f\"Pred Mask[{k}]\")\n",
    "    # plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ix1,ix2,ix3,ix4,ix5,ix6,ix7)=plt.subplots(1,7)\n",
    "ix1.imshow(im_test[0])\n",
    "\n",
    "res=r=model.predict(im_test[None,...][0])\n",
    "ix2.imshow(msk_test[0][0])\n",
    "ix3.imshow(msk_test[1][0])\n",
    "ix4.imshow(msk_test[2][0])\n",
    "# print(res)\n",
    "ix5.imshow(res[0][0])\n",
    "ix6.imshow(res[1][0])\n",
    "ix7.imshow(res[2][0])\n",
    "plt.show()\n",
    "\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy_k = history.history.keys()\n",
    "\n",
    "\n",
    "# Plot the epoch vs val_loss\n",
    "ax1 = plt.subplot(111)\n",
    "for _ in val_accuracy_k:\n",
    "    if _=='val_loss':\n",
    "        ax2 = ax1.twinx()\n",
    "        val_loss = history.history[_]\n",
    "        ax2.plot(epochs_list, val_loss, 'k-')\n",
    "        ax2.set_ylabel('Validation Loss', color='b')\n",
    "    else:\n",
    "\n",
    "        ax1.plot(epochs_list, history.history[_], 'r-')\n",
    "    # ax1.set_ylabel('Validation Accuracy', color='r')\n",
    "ax1.set_ylim([0.0, 1])\n",
    "ax2.set_ylim([0.0, 10])\n",
    "\n",
    "# Create a second y-axis for the val_accuracy\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.plot(epochs_list, val_loss, 'k-')\n",
    "# ax2.set_ylabel('Validation Loss', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_path=f\"weights/{modelname}.weights.h5\"\n",
    "model.save_weights(weights_path)\n",
    "\n",
    "model_path=f\"models/{modelname}.keras\"\n",
    "model.save(model_path)\n",
    "print(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json_path=\"models/unet-128-single-model-6ch.json\"\n",
    "model.to_json()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
